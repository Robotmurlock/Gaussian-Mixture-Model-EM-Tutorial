# Gaussian Mixture Model and Expectation Maximization

![](https://camo.githubusercontent.com/0bb39f67ea4bfdd2b6a48fefc5486568f18e79e0377b936c8096f023958bfedb/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f362f36392f454d5f436c7573746572696e675f6f665f4f6c645f466169746866756c5f646174612e676966)

Implementation and comprehensive explanation of [Gaussian Mixture Model](https://en.wikipedia.org/wiki/Mixture_model) generative probabilistic algorithm using [Expectation Maximization](https://en.wikipedia.org/wiki/Expectationâ€“maximization_algorithm) optimization method. Tutorial is represented in [Jupyter notebook](https://github.com/Robotmurlock/GaussianMixtureModel-EM-Tutorial/blob/main/notebook.ipynb). It can also be found on [Kaggle](https://www.kaggle.com/code/momiradzemovic/gaussian-mixture-model-comprehensive-intro).

Expectation Maximization is a general method that can be used for parameter estimation that optimize maximum likelihood given the observed data with latent variables (data is not fully observed). It has many more applications (e.g. dynamic model parameter estimation) besides GMM but this is a good approach to learn intuition behind the optimization algorithm.

## Reference

Main reference for this tutorial is [Pattern Recognition and Machine Learning by Christopher M. Bishop](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf). This book available online (click on the link).

