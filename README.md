# Gaussian Mixture Model and Expectation Minimization

Implementation and comprehensive explanation of [Gaussian Mixture Model](https://en.wikipedia.org/wiki/Mixture_model) generative probabilistic algorithm using [Expectation Minimization](https://en.wikipedia.org/wiki/Expectationâ€“maximization_algorithm) optimization method. Tutorial is represented in [Jupyter notebook](https://github.com/Robotmurlock/GaussianMixtureModel-EM-Tutorial/blob/main/notebook.ipynb). It can also be found on [Kaggle](https://www.kaggle.com/code/momiradzemovic/gaussian-mixture-model-comprehensive-intro).

Expectation Minimization is a general method that can be used for parameter estimation that optimize maximum likelihood given the observed data with latent variables (data is not fully observed). It has many more applications (e.g. dynamic model parameter estimation) besides GMM but this is a good approach to learn intuition behind the optimization algorithm.

## Reference

Main reference for this tutorial is [Pattern Recognition and Machine Learning by Christopher M. Bishop](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf). This book available online (click on the link).

